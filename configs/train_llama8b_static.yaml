# python3 glp_train.py config=configs/train_llama8b_static.yaml
save_root: .
model_name: meta-llama/Meta-Llama-3.1-8B
run_name: glp-llama8b-d${glp_kwargs.denoiser_config.n_layers}_static-1M
output_path: ${save_root}/runs/${run_name}
# wandb
wandb_enabled: True
wandb_project: glp
wandb_run_name: ${run_name}
# data
train_dataset: ${save_root}/data/llama8b-layer15-fineweb-1M
rep_statistic: ${train_dataset}/rep_statistics.pt
num_epochs: 1
save_epochs: [1]
shuffle: True
# model
glp_kwargs:
  # these are training hparams
  normalizer_config:
    rep_statistic: ${rep_statistic}
  denoiser_config:
    d_input: 4096 # llm hidden dim
    d_model: 8192 # d_input * 2
    d_mlp: 16384 # d_input * 4
    n_layers: 6
    multi_layer_n_layers: null # used for multi-layer training
  # metadata describing training data
  # this is an annotation, not a training hparam
  tracedict_config:
    layer_prefix: "model.layers"
    layers: [15]
    retain: "output" # controls what to save; one of ["input", "output"]
# training
use_bf16: True
learning_rate: 5e-5
batch_size: 4096
gradient_accumulation_steps: 1
gradient_clipping_threshold: 1
log_every_n_steps: 10
save_opt_state: True
lr_scheduler:
  scheduler_cls: cosine_scheduler_with_warmup
  warmup_ratio: 0.01
  initial_factor: 0.01
  final_factor: 0.1